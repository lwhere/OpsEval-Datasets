{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from groq import Groq\n",
    "from openai import OpenAI\n",
    "openai_client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "groq_client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screening_prompt = \"\"\"\n",
    "I need your help in analyzing a multi-choice question, determine the domain and the task type it belongs to.\n",
    "Domains:\n",
    "When classifying the domain, be specific, dive deeper into domains such as:\n",
    "- Database\n",
    "- Network Operations\n",
    "Task Types:\n",
    "For the task type, consider categories like:\n",
    "- Monitoring and Alerts\n",
    "- Performance Optimization\n",
    "Summary your response as JSON format: {{\"domain\": \"specific_domain\", \"task\": \"specific_task_type\"}}\n",
    "\n",
    "The question is:\n",
    "{question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_single_choice_answer_prompt = \"\"\"\n",
    "你是一个运维领域的专家，我将给你一道单选题{question},请你输出该题目对应的唯一正确的选项，\n",
    "格式：直接输出答案选项，不要输出其他内容。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343\n",
      "0 question_answer\n",
      "343 multi_choice\n",
      "单项答案数量: 342\n",
      "两项答案: 0\n",
      "三项答案: 0\n",
      "other_choice_data: [{'id': '5G Communication-36', 'question': '下列哪个不是核心网实现切片的关键技术？\\nA: 可保障的SLA\\nB: 网络按需灵活自动分配\\nC: 网络资源隔离关键技术\\nD: 网络功能动态加载\\nE: 网络自愈', 'choices': ['可保障的SLA', '网络按需灵活自动分配', '网络资源隔离关键技术', '网络功能动态加载', '网络自愈'], 'qtype': 0, 'answer': 'E', 'solution': ''}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 343/343 [15:28<00:00,  2.71s/it]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "with open('5G Communication.json') as f:\n",
    "    data = json.load(f)\n",
    "question_answer = []\n",
    "multi_choice = []\n",
    "for item in data:\n",
    "    if 'solution' in item:\n",
    "        multi_choice.append(item)\n",
    "    else:\n",
    "        question_answer.append(item)\n",
    "print(len(data))\n",
    "print(f\"{len(question_answer)} question_answer\")\n",
    "print(f\"{len(multi_choice)} multi_choice\")\n",
    "# 初始化计数变量\n",
    "single_choice = 0\n",
    "two_choice = 0\n",
    "three_choice = 0\n",
    "three_choice_data = []\n",
    "other_choice_data = []\n",
    "\n",
    "# 遍历 JSON 数据\n",
    "for item in data:\n",
    "    answer = item['answer']\n",
    "\n",
    "    # 使用正则表达式检查选项数量\n",
    "    if re.match(r'^[A-D]$', ''.join(answer)):\n",
    "        single_choice += 1\n",
    "    elif re.match(r'^[A-D]{2}$', ''.join(answer)):\n",
    "        two_choice += 1\n",
    "    elif re.match(r'^[A-D]{3}$', ''.join(answer)):\n",
    "        three_choice += 1\n",
    "        three_choice_data.append(item)\n",
    "    else:\n",
    "        other_choice_data.append(item)\n",
    "\n",
    "# 输出结果\n",
    "print(f\"单项答案数量: {single_choice}\")\n",
    "print(f\"两项答案: {two_choice}\")\n",
    "print(f\"三项答案: {three_choice}\")\n",
    "print(f\"other_choice_data: {other_choice_data}\")\n",
    "for user_content_prompt in tqdm(multi_choice):\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": get_single_choice_answer_prompt.format(question=user_content_prompt['question']),\n",
    "        }]\n",
    "    llama3_chat_completion = groq_client.chat.completions.create(\n",
    "        model= \"llama3-8b-8192\",\n",
    "        messages=messages)\n",
    "    gemma_chat_completion = groq_client.chat.completions.create(\n",
    "        model= \"gemma-7b-it\",\n",
    "        messages=messages)\n",
    "    user_content_prompt[\"llama3-8b-answer\"] = llama3_chat_completion.choices[0].message.content\n",
    "    user_content_prompt[\"gemma-7b-answer\"] = gemma_chat_completion.choices[0].message.content\n",
    "with open(\"5G Communication_output.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(multi_choice, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n",
      "0 question_answer\n",
      "110 multi_choice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [04:15<00:00,  2.32s/it]\n"
     ]
    }
   ],
   "source": [
    "with open('Log Analysis_single_choice_data.json') as f:\n",
    "    data = json.load(f)\n",
    "question_answer = []\n",
    "multi_choice = []\n",
    "for item in data:\n",
    "    if 'solution' in item:\n",
    "        multi_choice.append(item)\n",
    "    else:\n",
    "        question_answer.append(item)\n",
    "print(len(data))\n",
    "print(f\"{len(question_answer)} question_answer\")\n",
    "print(f\"{len(multi_choice)} multi_choice\")\n",
    "\n",
    "for user_content_prompt in tqdm(multi_choice):\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": get_single_choice_answer_prompt.format(question=user_content_prompt['question']),\n",
    "        }]\n",
    "    llama3_chat_completion = groq_client.chat.completions.create(\n",
    "        model= \"llama3-8b-8192\",\n",
    "        messages=messages)\n",
    "    gemma_chat_completion = groq_client.chat.completions.create(\n",
    "        model= \"gemma-7b-it\",\n",
    "        messages=messages)\n",
    "    user_content_prompt[\"llama3-8b-answer\"] = llama3_chat_completion.choices[0].message.content\n",
    "    user_content_prompt[\"gemma-7b-answer\"] = gemma_chat_completion.choices[0].message.content\n",
    "with open(\"Log Analyis_single_choice_llama3_gemma.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(multi_choice, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n",
      "0 question_answer\n",
      "184 multi_choice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 184/184 [07:54<00:00,  2.58s/it]\n"
     ]
    }
   ],
   "source": [
    "with open('Oracle Database_single_choice_data.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "question_answer = []\n",
    "multi_choice = []\n",
    "for item in data:\n",
    "    if 'solution' in item:\n",
    "        multi_choice.append(item)\n",
    "    else:\n",
    "        question_answer.append(item)\n",
    "print(len(data))\n",
    "print(f\"{len(question_answer)} question_answer\")\n",
    "print(f\"{len(multi_choice)} multi_choice\")\n",
    "\n",
    "\n",
    "for user_content_prompt in tqdm(multi_choice):\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": get_single_choice_answer_prompt.format(question=user_content_prompt['question']),\n",
    "        }]\n",
    "    llama3_chat_completion = groq_client.chat.completions.create(\n",
    "        model= \"llama3-8b-8192\",\n",
    "        messages=messages)\n",
    "    gemma_chat_completion = groq_client.chat.completions.create(\n",
    "        model= \"gemma-7b-it\",\n",
    "        messages=messages)\n",
    "    user_content_prompt[\"llama3-8b-answer\"] = llama3_chat_completion.choices[0].message.content\n",
    "    user_content_prompt[\"gemma-7b-answer\"] = gemma_chat_completion.choices[0].message.content\n",
    "with open(\"Oracle Database_single_choice_data_lama3_gemma.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(multi_choice, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 38/1262 [00:59<49:39,  2.43s/it] "
     ]
    }
   ],
   "source": [
    "# file_name = 'Wired Network.json'\n",
    "# with open(file_name) as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# question_answer = []\n",
    "# multi_choice = []\n",
    "# for item in data:\n",
    "#     if 'solution' in item:\n",
    "#         multi_choice.append(item)\n",
    "#     else:\n",
    "#         question_answer.append(item)\n",
    "# print(len(data))\n",
    "# print(f\"{len(question_answer)} question_answer\")\n",
    "# print(f\"{len(multi_choice)} multi_choice\")\n",
    "\n",
    "\n",
    "# # 初始化计数变量\n",
    "# single_choice = 0\n",
    "# two_choice = 0\n",
    "# three_choice = 0\n",
    "# four_choice = 0\n",
    "# single_choice_data = []\n",
    "# three_choice_data = []\n",
    "# four_choice_data = []\n",
    "# other_choice_data = []\n",
    "\n",
    "# # 遍历 JSON 数据\n",
    "# for item in data:\n",
    "#     answer = item['answer']\n",
    "\n",
    "#     # 使用正则表达式检查选项数量\n",
    "#     if re.match(r'^[A-G]$', ''.join(answer)):\n",
    "#         single_choice += 1\n",
    "#         single_choice_data.append(item)\n",
    "#     elif re.match(r'^[A-G](,?[A-G]){1}$', ''.join(answer)):\n",
    "#         two_choice += 1\n",
    "#     elif re.match(r'^[A-G](,?[A-G]){2}$', ''.join(answer)):\n",
    "#         three_choice += 1\n",
    "#         three_choice_data.append(item)\n",
    "#     elif re.match(r'^[A-G](,?[A-G]){3}$', ''.join(answer)):\n",
    "#         four_choice += 1\n",
    "#         four_choice_data.append(item)\n",
    "#     else:\n",
    "#         other_choice_data.append(item)\n",
    "\n",
    "# # 输出结果\n",
    "# print(f\"单项答案数量: {single_choice}\")\n",
    "# print(f\"两项答案: {two_choice}\")\n",
    "# print(f\"三项答案: {three_choice}\")\n",
    "# print(f\"四项答案: {four_choice}\")\n",
    "# print(f\"other_choice_data: {len(other_choice_data)}\")\n",
    "\n",
    "\n",
    "# with open(file_name.split('.j')[0]+\"_other_choice_data\"+\".json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(other_choice_data, f, ensure_ascii=False)\n",
    "\n",
    "# def format_questions(data_list):\n",
    "#     letters = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]\n",
    "\n",
    "#     for data in data_list:\n",
    "#         formatted_question = data[\"question\"] + \"\\n\\n\"\n",
    "#         for i, choice in enumerate(data[\"choices\"]):\n",
    "#             formatted_question += f\"{letters[i]}. {choice} \"\n",
    "#         data[\"question\"] = formatted_question\n",
    "    \n",
    "#     return data_list\n",
    "\n",
    "# single_choice_data = format_questions(single_choice_data)\n",
    "\n",
    "# with open(file_name.split('.j')[0]+\"_single_choice_data\"+\".json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(single_choice_data, f, ensure_ascii=False)\n",
    "\n",
    "with open(\"Wired Network_single_choice_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    single_choice_data = json.load(f)\n",
    "temp_cache = []\n",
    "idx = 0\n",
    "for user_content_prompt in tqdm(single_choice_data):\n",
    "    idx += 1\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": get_single_choice_answer_prompt.format(question=user_content_prompt['question']),\n",
    "        }]\n",
    "    llama3_chat_completion = groq_client.chat.completions.create(\n",
    "        model= \"llama3-8b-8192\",\n",
    "        messages=messages,\n",
    "        temperature=0)\n",
    "    gemma_chat_completion = groq_client.chat.completions.create(\n",
    "        model= \"gemma-7b-it\",\n",
    "        messages=messages,\n",
    "        temperature=0)\n",
    "    user_content_prompt[\"llama3-8b-answer\"] = llama3_chat_completion.choices[0].message.content\n",
    "    user_content_prompt[\"gemma-7b-answer\"] = gemma_chat_completion.choices[0].message.content\n",
    "    temp_cache.append(user_content_prompt)\n",
    "    if idx % 50 == 0:\n",
    "        with open(\"cache.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(temp_cache, f, ensure_ascii=False)\n",
    "with open(\"Wired_network_single_choice_data_lama3_gemma_temperation=0.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(single_choice_data, f, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
